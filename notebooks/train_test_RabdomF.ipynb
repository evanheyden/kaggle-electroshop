# Imports standards
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import warnings
import joblib
import os

warnings.filterwarnings("ignore")

# Configuration affichage
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 100)
sns.set_style("whitegrid")

print("‚úì Imports OK")
# Charger les donn√©es
df = pd.read_csv(
    "../data/raw/dsba-m-1-challenge-purchase-prediction/test_dataset_M1_with_id.csv"
)

print(f"üìä Dataset shape: {df.shape}")
print(f"üìÖ P√©riode: Day {df['Day'].min()} √† {df['Day'].max()}")
print(f"üéØ Target balance: {df['Purchase'].value_counts(normalize=True)}")

# Afficher les premi√®res lignes
display(df.head())

# %%
# V√©rifier les types de colonnes
print("üîç Types de donn√©es:")
display(df.dtypes)
# Statistiques de valeurs manquantes
print("‚ùå Valeurs manquantes par colonne:")
missing = df.isnull().sum()
missing_pct = (missing / len(df) * 100).round(2)
missing_df = pd.DataFrame({"Missing_Count": missing, "Missing_Pct": missing_pct})
display(
    missing_df[missing_df["Missing_Count"] > 0].sort_values(
        "Missing_Pct", ascending=False
    )
)
# V√©rifier les valeurs uniques des cat√©gorielles
print("üìÇ Cat√©gories uniques:")
for col in df.select_dtypes(include="object").columns:
    print(f"\n{col}: {df[col].nunique()} valeurs")
    print(f"  ‚Üí {df[col].unique()[:10]}")
# Split temporel
train = df[df["Day"] <= 60].copy()
val = df[(df["Day"] > 60) & (df["Day"] <= 70)].copy()
test = df[df["Day"] > 70].copy()

print(f"üì¶ Train: {len(train):,} lignes ({len(train)/len(df)*100:.1f}%)")
print(f"üì¶ Val:   {len(val):,} lignes ({len(val)/len(df)*100:.1f}%)")
print(f"üì¶ Test:  {len(test):,} lignes ({len(test)/len(df)*100:.1f}%)")
# V√©rifier la distribution de la target
print("\nüéØ Distribution Purchase:")
print(f"  Train: {train['Purchase'].mean():.2%}")
print(f"  Val:   {val['Purchase'].mean():.2%}")
if len(test) > 0 and "Purchase" in test.columns:
    print(f"  Test:  {test['Purchase'].mean():.2%}")
def engineer_features(df, campaign_days=None, is_train=True):
    """
    Cr√©e toutes les features engineer√©es

    Parameters:
    -----------
    df : DataFrame
        Donn√©es √† transformer
    campaign_days : list
        Liste des jours de campagne (appris sur train)
    is_train : bool
        Si True, apprend les campaign_days

    Returns:
    --------
    df : DataFrame transform√©
    campaign_days : list (si is_train=True)
    """

    df = df.copy()

    # === 1. FEATURES PRIX ===
    print("üí∞ Features prix...")
    # Montant r√©el de la r√©duction
    df["Effective_Discount"] = df["Price"] * df["Discount"] / 100

    # Prix apr√®s r√©duction
    df["Net_Price"] = df["Price"] * (1 - df["Discount"] / 100)

    # Buckets de prix (robuste aux outliers)
    df["Price_Bucket"] = pd.qcut(df["Price"], q=5, labels=False, duplicates="drop")

    # === 2. FEATURES ENGAGEMENT ===
    print("üéØ Features engagement...")
    # Interaction email √ó score d'engagement
    df["Email_x_Engagement"] = df["Email_Interaction"] * df["Engagement_Score"]

    # Items dans le panier √ó engagement
    df["Cart_x_Engagement"] = df["Items_In_Cart"] * df["Engagement_Score"]

    # === 3. FEATURES CAMPAGNE (insights EDA) ===
    print("üì¢ Features campagne...")
    # Tablet pendant campagne (tr√®s performant selon EDA)
    df["Tablet_During_Campaign"] = (
        (df["Device_Type"] == "Tablet") & (df["Campaign_Period"])
    ).astype(int)

    # Desktop pendant campagne
    df["Desktop_During_Campaign"] = (
        (df["Device_Type"] == "Desktop") & (df["Campaign_Period"])
    ).astype(int)

    # Distance au campaign le plus proche
    if is_train:
        # Apprendre quels jours sont des campagnes
        campaign_days = sorted(df[df["Campaign_Period"] == True]["Day"].unique())
        print(
            f"  üìÖ Jours de campagne d√©tect√©s: {campaign_days[:5]}... ({len(campaign_days)} jours)"
        )

    if campaign_days is not None and len(campaign_days) > 0:
        df["Day_to_Campaign"] = df["Day"].apply(
            lambda d: min([abs(d - cd) for cd in campaign_days])
        )
    else:
        df["Day_to_Campaign"] = 999  # Valeur par d√©faut si pas de campagne

    # === 4. FEATURES CAT√âGORIELLES ===
    print("üìÇ Features cat√©gorielles...")
    # Cat√©gories haute valeur (0, 1, 2 selon EDA)
    df["HighValue_Category"] = df["Category"].isin([0.0, 1.0, 2.0]).astype(int)

    # Email + Device premium (Tablet/Desktop)
    df["Email_Device_High"] = (
        (df["Email_Interaction"] == 1.0)
        & (df["Device_Type"].isin(["Tablet", "Desktop"]))
    ).astype(int)

    print(f"‚úì {len(df.columns)} colonnes apr√®s feature engineering")

    if is_train:
        return df, campaign_days
    else:
        return df
# Cr√©er les features sur TRAIN d'abord (pour apprendre campaign_days)
print("=" * 60)
print("FEATURE ENGINEERING - TRAIN")
print("=" * 60)
train_featured, campaign_days = engineer_features(train, is_train=True)
# Puis sur VAL et TEST en utilisant les campaign_days du train
print("\n" + "=" * 60)
print("FEATURE ENGINEERING - VAL")
print("=" * 60)
val_featured = engineer_features(val, campaign_days=campaign_days, is_train=False)

if len(test) > 0:
    print("\n" + "=" * 60)
    print("FEATURE ENGINEERING - TEST")
    print("=" * 60)
    test_featured = engineer_features(test, campaign_days=campaign_days, is_train=False)
else:
    test_featured = test.copy()
# V√©rifier les nouvelles features
print("üÜï Nouvelles features cr√©√©es:")
new_features = [col for col in train_featured.columns if col not in df.columns]
print(new_features)

# Afficher quelques statistiques
display(train_featured[new_features].describe())
def add_missing_indicators(df, cols_with_missing=None, is_train=True, threshold=0.01):
    """
    Ajoute des indicateurs de valeurs manquantes

    Parameters:
    -----------
    df : DataFrame
    cols_with_missing : list
        Colonnes √† traiter (appris sur train)
    is_train : bool
        Si True, d√©tecte les colonnes avec >threshold missing
    threshold : float
        Seuil de % de missing pour cr√©er un flag

    Returns:
    --------
    df : DataFrame avec flags
    cols_with_missing : list
    """

    df = df.copy()

    if is_train:
        # D√©tecter les colonnes avec >threshold de missing
        missing_pct = df.isnull().sum() / len(df)
        cols_with_missing = missing_pct[missing_pct > threshold].index.tolist()
        print(f"üìç {len(cols_with_missing)} colonnes avec >{threshold*100}% missing:")
        for col in cols_with_missing:
            pct = missing_pct[col] * 100
            print(f"  - {col}: {pct:.2f}%")

    # Ajouter les flags
    if cols_with_missing:
        for col in cols_with_missing:
            if col in df.columns:
                df[f"{col}_missing"] = df[col].isnull().astype(int)

    if is_train:
        return df, cols_with_missing
    else:
        return df
# Ajouter les missing indicators
print("=" * 60)
print("MISSING INDICATORS")
print("=" * 60)

train_featured, cols_with_missing = add_missing_indicators(
    train_featured, is_train=True, threshold=0.01
)

val_featured = add_missing_indicators(
    val_featured, cols_with_missing=cols_with_missing, is_train=False
)

if len(test_featured) > 0:
    test_featured = add_missing_indicators(
        test_featured, cols_with_missing=cols_with_missing, is_train=False
    )

print(f"\n‚úì Flags ajout√©s pour: {cols_with_missing}")
def impute_numeric_features(train, val, test=None):
    """
    Impute les features num√©riques avec la m√©diane du train

    Returns:
    --------
    train, val, test (imput√©s)
    impute_values (dict des m√©dianes)
    """

    train = train.copy()
    val = val.copy()
    if test is not None:
        test = test.copy()

    # Identifier les colonnes num√©riques avec des NaN
    numeric_cols = train.select_dtypes(include=[np.number]).columns
    cols_to_impute = [col for col in numeric_cols if train[col].isnull().any()]

    print(f"üî¢ Imputation de {len(cols_to_impute)} colonnes num√©riques:")

    impute_values = {}

    for col in cols_to_impute:
        # Calculer la m√©diane sur TRAIN uniquement
        median_val = train[col].median()
        impute_values[col] = median_val

        # Appliquer sur train, val, test
        train[col] = train[col].fillna(median_val)
        val[col] = val[col].fillna(median_val)
        if test is not None:
            if col in test.columns:  # <-- Ajoutez cette v√©rification
                test[col] = test[col].fillna(median_val)

        print(f"  - {col}: m√©diane = {median_val:.2f}")

    print(f"\n‚úì Imputation termin√©e")

    if test is not None:
        return train, val, test, impute_values
    else:
        return train, val, None, impute_values
def impute_categorical_features(train, val, test=None):
    """
    Impute les features cat√©gorielles avec 'Unknown'
    """

    train = train.copy()
    val = val.copy()
    if test is not None:
        test = test.copy()

    cat_cols = train.select_dtypes(include="object").columns
    cols_to_impute = [col for col in cat_cols if train[col].isnull().any()]

    print(f"üìÇ Imputation de {len(cols_to_impute)} colonnes cat√©gorielles:")

    for col in cols_to_impute:
        missing_count = train[col].isnull().sum()
        train[col] = train[col].fillna("Unknown")
        val[col] = val[col].fillna("Unknown")
        if test is not None:
            test[col] = test[col].fillna("Unknown")

        print(f"  - {col}: {missing_count} valeurs ‚Üí 'Unknown'")

    print(f"‚úì Imputation termin√©e")

    if test is not None:
        return train, val, test
    else:
        return train, val, None
# Imputation
print("=" * 60)
print("IMPUTATION - NUM√âRIQUES")
print("=" * 60)

train_featured, val_featured, test_featured, impute_vals = impute_numeric_features(
    train_featured, val_featured, test_featured
)

print("\n" + "=" * 60)
print("IMPUTATION - CAT√âGORIELLES")
print("=" * 60)

train_featured, val_featured, test_featured = impute_categorical_features(
    train_featured, val_featured, test_featured
)
# V√©rifier qu'il n'y a plus de NaN
print("üîç V√©rification des valeurs manquantes apr√®s imputation:")
print(f"  Train: {train_featured.isnull().sum().sum()} NaN")
print(f"  Val:   {val_featured.isnull().sum().sum()} NaN")
if len(test_featured) > 0:
    print(f"  Test:  {test_featured.isnull().sum().sum()} NaN")
def handle_rare_categories(train, val, test=None, cat_features=None, threshold=0.01):
    """
    Groupe les cat√©gories rares (<threshold) en 'Other'

    Parameters:
    -----------
    threshold : float
        % minimum pour garder une cat√©gorie

    Returns:
    --------
    train, val, test (transform√©s)
    known_categories (dict)
    """

    train = train.copy()
    val = val.copy()
    if test is not None:
        test = test.copy()

    if cat_features is None:
        cat_features = train.select_dtypes(include="object").columns.tolist()

    known_categories = {}

    print(f"üìä Groupement des cat√©gories rares (<{threshold*100}%):")

    for col in cat_features:
        if col not in train.columns:
            continue

        # Compter les occurrences dans train
        value_counts = train[col].value_counts()
        freq = value_counts / len(train)

        # Garder seulement les cat√©gories fr√©quentes
        valid_cats = freq[freq >= threshold].index.tolist()
        known_categories[col] = valid_cats

        rare_count = len(value_counts) - len(valid_cats)

        if rare_count > 0:
            print(f"  - {col}: {rare_count}/{len(value_counts)} cat√©gories ‚Üí 'Other'")

            # Remplacer les rares par 'Other'
            train[col] = train[col].apply(lambda x: x if x in valid_cats else "Other")
            val[col] = val[col].apply(lambda x: x if x in valid_cats else "Other")
            if test is not None:
                test[col] = test[col].apply(lambda x: x if x in valid_cats else "Other")

    print(f"\n‚úì Groupement termin√©")

    if test is not None:
        return train, val, test, known_categories
    else:
        return train, val, None, known_categories
# D√©finir les features cat√©gorielles
cat_features = [
    "Device_Type",
    "Time_of_Day",
    "Payment_Method",
    "Referral_Source",
    "Category",
]

print("=" * 60)
print("RARE CATEGORIES HANDLING")
print("=" * 60)

train_featured, val_featured, test_featured, known_cats = handle_rare_categories(
    train_featured,
    val_featured,
    test_featured,
    cat_features=cat_features,
    threshold=0.01,
)

# Afficher les cat√©gories gard√©es
print("\nüìã Cat√©gories valides par feature:")
for col, cats in known_cats.items():
    print(f"  {col}: {cats}")
def prepare_catboost_data(
    train,
    val,
    test,
    cat_features,
    target_col="Purchase",
    id_col="id",
    session_col="Session_ID",
):
    """
    Pr√©pare les donn√©es pour CatBoost

    Returns:
    --------
    X_train, y_train, X_val, y_val, X_test, cat_indices
    """

    # Convertir les cat√©gorielles en string
    train = train.copy()
    val = val.copy()
    test = test.copy() if test is not None and len(test) > 0 else None

    for col in cat_features:
        if col in train.columns:
            train[col] = train[col].astype(str)
            val[col] = val[col].astype(str)
            if test is not None:
                test[col] = test[col].astype(str)

    # Colonnes √† drop
    drop_cols = [target_col, id_col, session_col]
    drop_cols = [c for c in drop_cols if c in train.columns]

    # S√©parer features et target
    X_train = train.drop(columns=drop_cols)
    y_train = train[target_col]

    X_val = val.drop(columns=drop_cols)
    y_val = val[target_col] if target_col in val.columns else None

    if test is not None:
        X_test = (
            test.drop(columns=drop_cols)
            if target_col in test.columns
            else test.drop(
                columns=[c for c in [id_col, session_col] if c in test.columns]
            )
        )
    else:
        X_test = None

    # Indices des colonnes cat√©gorielles
    cat_indices = [
        X_train.columns.get_loc(col) for col in cat_features if col in X_train.columns
    ]

    print("üì¶ CatBoost Data Ready:")
    print(f"  X_train: {X_train.shape}")
    print(f"  X_val:   {X_val.shape}")
    if X_test is not None:
        print(f"  X_test:  {X_test.shape}")
    print(f"  Categorical indices: {cat_indices}")
    print(f"  Categorical features: {[X_train.columns[i] for i in cat_indices]}")

    return X_train, y_train, X_val, y_val, X_test, cat_indices


print("=" * 60)
print("PIPELINE 1 : CATBOOST")
print("=" * 60)

X_train_cb, y_train_cb, X_val_cb, y_val_cb, X_test_cb, cat_indices = (
    prepare_catboost_data(
        train_featured, val_featured, test_featured, cat_features=cat_features
    )
)

# Afficher les premi√®res lignes
print("\nüìä Aper√ßu X_train_cb:")
display(X_train_cb.head())

# V√©rifier les types
print("\nüîç Types de colonnes:")
print(X_train_cb.dtypes.value_counts())
def prepare_classic_data(
    train,
    val,
    test,
    cat_features,
    scale=True,
    target_col="Purchase",
    id_col="id",
    session_col="Session_ID",
):
    """
    Pr√©pare les donn√©es pour mod√®les classiques (one-hot + scaling)

    Returns:
    --------
    X_train, y_train, X_val, y_val, X_test, scaler
    """

    train = train.copy()
    val = val.copy()
    test = test.copy() if test is not None and len(test) > 0 else None

    # Colonnes √† drop
    drop_cols = [target_col, id_col, session_col]
    drop_cols = [c for c in drop_cols if c in train.columns]

    # S√©parer features et target
    X_train = train.drop(columns=drop_cols)
    y_train = train[target_col]

    X_val = val.drop(columns=drop_cols)
    y_val = val[target_col] if target_col in val.columns else None

    if test is not None:
        X_test = (
            test.drop(columns=drop_cols)
            if target_col in test.columns
            else test.drop(
                columns=[c for c in [id_col, session_col] if c in test.columns]
            )
        )
    else:
        X_test = None

    print("üîÑ One-hot encoding...")

    # One-hot encoding
    X_train_encoded = pd.get_dummies(X_train, columns=cat_features, drop_first=True)
    X_val_encoded = pd.get_dummies(X_val, columns=cat_features, drop_first=True)
    if X_test is not None:
        X_test_encoded = pd.get_dummies(X_test, columns=cat_features, drop_first=True)
    else:
        X_test_encoded = None

    # Aligner les colonnes (val/test doivent avoir les m√™mes que train)
    train_cols = X_train_encoded.columns.tolist()

    # Ajouter les colonnes manquantes dans val
    missing_in_val = set(train_cols) - set(X_val_encoded.columns)
    for col in missing_in_val:
        X_val_encoded[col] = 0

    # Supprimer les colonnes en trop dans val
    extra_in_val = set(X_val_encoded.columns) - set(train_cols)
    X_val_encoded = X_val_encoded.drop(columns=list(extra_in_val))

    # R√©ordonner les colonnes
    X_val_encoded = X_val_encoded[train_cols]

    # M√™me chose pour test
    if X_test_encoded is not None:
        missing_in_test = set(train_cols) - set(X_test_encoded.columns)
        for col in missing_in_test:
            X_test_encoded[col] = 0

        extra_in_test = set(X_test_encoded.columns) - set(train_cols)
        X_test_encoded = X_test_encoded.drop(columns=list(extra_in_test))

        X_test_encoded = X_test_encoded[train_cols]

    print(f"  ‚úì Train: {X_train_encoded.shape[1]} features apr√®s encoding")
    print(
        f"  ‚úì Val:   {len(missing_in_val)} colonnes ajout√©es, {len(extra_in_val)} supprim√©es"
    )
    if X_test_encoded is not None:
        print(
            f"  ‚úì Test:  {len(missing_in_test)} colonnes ajout√©es, {len(extra_in_test)} supprim√©es"
        )

    # Scaling
    scaler = None
    if scale:
        print("\nüìè Scaling (StandardScaler)...")
        scaler = StandardScaler()

        # Fit sur train uniquement
        X_train_encoded = pd.DataFrame(
            scaler.fit_transform(X_train_encoded),
            columns=X_train_encoded.columns,
            index=X_train_encoded.index,
        )

        # Transform sur val et test
        X_val_encoded = pd.DataFrame(
            scaler.transform(X_val_encoded),
            columns=X_val_encoded.columns,
            index=X_val_encoded.index,
        )

        if X_test_encoded is not None:
            X_test_encoded = pd.DataFrame(
                scaler.transform(X_test_encoded),
                columns=X_test_encoded.columns,
                index=X_test_encoded.index,
            )

        print("  ‚úì Scaling termin√©")

    print("\nüì¶ Classic ML Data Ready:")
    print(f"  X_train: {X_train_encoded.shape}")
    print(f"  X_val:   {X_val_encoded.shape}")
    if X_test_encoded is not None:
        print(f"  X_test:  {X_test_encoded.shape}")

    return X_train_encoded, y_train, X_val_encoded, y_val, X_test_encoded, scaler


print("=" * 60)
print("PIPELINE 2 : CLASSIC ML (ONE-HOT + SCALING)")
print("=" * 60)

X_train_cls, y_train_cls, X_val_cls, y_val_cls, X_test_cls, scaler = (
    prepare_classic_data(
        train_featured,
        val_featured,
        test_featured,
        cat_features=cat_features,
        scale=True,
    )
)

# Afficher les premi√®res lignes
print("\nüìä Aper√ßu X_train_cls:")
display(X_train_cls.head())

# V√©rifier la distribution (devrait √™tre ~N(0,1) apr√®s scaling)
print("\nüìä Statistiques apr√®s scaling:")
display(X_train_cls.describe().loc[["mean", "std"]].T.head(10))


# Test rapide Logistic Regression sur le pipeline classic

print("=" * 60)
print("VALIDATION RAPIDE - LOGISTIC REGRESSION")
print("=" * 60)
# === SAUVEGARDE DES PIPELINES ===
print("=" * 60)
print("SAUVEGARDE DES DONN√âES PREPROCESSED")
print("=" * 60)

import os

os.makedirs("../data/processed", exist_ok=True)

# Donn√©es CatBoost
catboost_data = {
    "X_train": X_train_cb,
    "y_train": y_train_cb,
    "X_val": X_val_cb,
    "y_val": y_val_cb,
    "X_test": X_test_cb,
    "cat_indices": cat_indices,
    "cat_features_names": [col for col in X_train_cb.columns if col in cat_features],
}

joblib.dump(catboost_data, "../data/processed/catboost_ready_test.pkl", protocol=4)
print("‚úÖ Donn√©es CatBoost export√©es dans 'catboost_ready.pkl'")

# Donn√©es Classic ML
classic_data = {
    "X_train": X_train_cls,
    "y_train": y_train_cls,
    "X_val": X_val_cls,
    "y_val": y_val_cls,
    "X_test": X_test_cls,
    "scaler": scaler,
}

joblib.dump(classic_data, "../data/processed/classic_ready_test.pkl", protocol=4)
print("‚úÖ Donn√©es Classic ML export√©es dans 'classic_ready.pkl'")

# Afficher un r√©sum√© (avec gestion de None)
print("\n" + "=" * 60)
print("R√âSUM√â DES SAUVEGARDES")
print("=" * 60)
print(f"üì¶ CatBoost ready:")
print(f"   Train: {X_train_cb.shape}")
print(f"   Val: {X_val_cb.shape}")
if X_test_cb is not None:
    print(f"   Test: {X_test_cb.shape}")
else:
    print(f"   Test: Non disponible")
print(f"   Cat indices: {cat_indices}")

print(f"\nüì¶ Classic ready:")
print(f"   Train: {X_train_cls.shape}")
print(f"   Val: {X_val_cls.shape}")
if X_test_cls is not None:
    print(f"   Test: {X_test_cls.shape}")
else:
    print(f"   Test: Non disponible")
